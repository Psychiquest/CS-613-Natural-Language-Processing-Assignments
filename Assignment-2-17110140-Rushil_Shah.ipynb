{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.random import multinomial\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "import re,pickle,random\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "from sklearn.model_selection import train_test_split\n",
    "from math import log\n",
    "import gensim\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import SimpleRNN, Activation, Dense\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file):\n",
    "    \n",
    "    a=open(file,\"r\")\n",
    "    speech_text=a.read()\n",
    "    speech_text=re.sub(r'SPEECH \\d+','',speech_text)\n",
    "    speech_text=re.sub(r'\\d+/\\d+/\\d+','',speech_text)\n",
    "    speech_text=re.sub(r\";|-|\\.\\.+|\\?|'|–|``|’|,|\\$|\",'',speech_text)\n",
    "    speech_text=speech_text.replace(\"\\n\",\"\")\n",
    "    speech_text=speech_text.replace('\"','')\n",
    "    speech_text=re.sub(\"\\.\\S+\",\". \",speech_text)\n",
    "    speech_text=speech_text.lower()[1:]\n",
    "    \n",
    "    b=open(\"speech_filtered.txt\",\"w\")\n",
    "    b.write(speech_text)\n",
    "    a.close()\n",
    "    b.close()\n",
    "    \n",
    "preprocess(\"speeches.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_text=open(\"speech_filtered.txt\",\"r\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing sentences into list of list of tokenized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(speech_text):\n",
    "    tokenized_sent_list=sent_tokenize(speech_text)\n",
    "    newlist=[]\n",
    "    for sent in tokenized_sent_list:\n",
    "        sentnew=[\"<s>\"]\n",
    "        sentnew.extend(word_tokenize(sent))\n",
    "        sentnew.append(\"<\\s>\")\n",
    "        newlist.append(sentnew)\n",
    "    return newlist\n",
    "\n",
    "tokenized_sent_list=tokenize_sentence(speech_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating test data with randomized splits as 20 percent sentences of the data\n",
    "train_data, test_data = train_test_split(tokenized_sent_list, test_size=0.20, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating MLE of the NGram using train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_mle(n,tokenized_sent_list):\n",
    "    \n",
    "    if n==1:\n",
    "        ngramdict=Counter([])\n",
    "        \n",
    "        #NGramDict contains the frequency of every single unigram\n",
    "        \n",
    "        for sent in tokenized_sent_list:\n",
    "            unigram=list(ngrams(sent,n))\n",
    "            ngramdict+=Counter(unigram)\n",
    "            \n",
    "        total_tokens=sum(ngramdict.values())\n",
    "        \n",
    "        mle_dict={}\n",
    "        for sent in ngramdict.keys():\n",
    "            mle_dict[sent]=ngramdict[sent]/total_tokens #In Unigram, prob = count divided by total number of tokens\n",
    "        return mle_dict\n",
    "        \n",
    "    \n",
    "    # For all other ngrams it is count of ngram divided by count of n-1 gram of last n-1 words.\n",
    "    \n",
    "    ngramdict,n1gramdict=Counter([]),Counter([])\n",
    "    \n",
    "    for sent in tokenized_sent_list:\n",
    "        \n",
    "        ngram = list(ngrams(sent,n))\n",
    "        n1gram = list(ngrams(sent,n-1))\n",
    "        ngramdict += Counter(ngram)\n",
    "        n1gramdict += Counter(n1gram)\n",
    "        \n",
    "    ngramdict,n1gramdict=dict(ngramdict),dict(n1gramdict)\n",
    "\n",
    "    mle_dict={}\n",
    "    for sent in ngramdict.keys():\n",
    "        mle_dict[sent]=ngramdict[sent]/n1gramdict[sent[:-1]]\n",
    "\n",
    "    return mle_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(n,mle_dict):\n",
    "    \n",
    "    if n==1:\n",
    "        generated_sent=[\"<s>\"]       # For unigram, the multinomial distribution would be used on the entire corpus \n",
    "                                     # to predict the next word with the probabilites obtained from the mle_dict\n",
    "        generated_word=\"<s>\"\n",
    "        all_ngrams=list(mle_dict.keys())\n",
    "        div=sum(mle_dict.values())\n",
    "        prob=[mle_dict[x]/div for x in all_ngrams]\n",
    "\n",
    "        while ((generated_word!=\"<\\s>\") and (len(generated_sent)<25)):\n",
    "            a = np.random.multinomial(1, prob, size=1).tolist()\n",
    "            word_index = a[0].index(1)\n",
    "            generated_word=all_ngrams[word_index][0]\n",
    "            if generated_word!=\"<s>\" and generated_word!=\".\":\n",
    "                generated_sent.append(generated_word)\n",
    "        return generated_sent\n",
    "            \n",
    "        \n",
    "    \n",
    "    generated_sent,all_ngrams,all_probs=[],[],[]\n",
    "    \n",
    "    for sent in mle_dict.keys():\n",
    "        if sent[0] == '<s>':\n",
    "            all_ngrams.append(sent)\n",
    "            all_probs.append(mle_dict[sent])\n",
    "            \n",
    "    prob=[x/sum(all_probs) for x in all_probs]         \n",
    "    a = multinomial(1, prob, size=1).tolist()  # Using the multinomial distribution to get the start n words, so that \n",
    "                                               # it can be used to generate further sequence.\n",
    "    word_index = a[0].index(1)\n",
    "    for i in all_ngrams[word_index]:\n",
    "        generated_sent.append(i)\n",
    "\n",
    "    generated_word=generated_sent[-1]\n",
    "    \n",
    "    while ((generated_word!=\"<\\s>\") and (len(generated_sent)<25)): # Generated sequence will end when we find a </s>\n",
    "                                                                   # or the length of sentence exceeds 25 words.\n",
    "        all_ngrams,all_probs=[],[]\n",
    "        for sent in mle_dict.keys():\n",
    "            if (list(sent[:-1]))==generated_sent[len(generated_sent)-n+1:]:\n",
    "                all_ngrams.append(sent)\n",
    "                all_probs.append(mle_dict[sent])\n",
    "                \n",
    "        prob=[x/sum(all_probs) for x in all_probs] \n",
    "        a = multinomial(1, prob, size=1).tolist()\n",
    "        word_index = a[0].index(1)\n",
    "        generated_word=all_ngrams[word_index][-1]\n",
    "        \n",
    "        if generated_word!=\"<s>\":    # If somehow we get a <s> in the middle we have to ignore it.\n",
    "            generated_sent.append(generated_word)\n",
    "            \n",
    "    return generated_sent\n",
    "        \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity Score of Ngram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_perplexity(sentence,mle_dict,n):\n",
    "    p=0\n",
    "    for i in range(n-1,len(sentence)):\n",
    "        nwords=tuple(sentence[i-n+1:i+1])\n",
    "        try:\n",
    "            p+=log(mle_dict[nwords])   # Perplexity score is sum of log of probabilites of predicting each ngram\n",
    "        except:    # If the word is not present in the training data, ignore it!\n",
    "            pass\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_perplexity_test_data(test_data,mle_dict,n): # This function would iterate over the complete test corpus \n",
    "                                                     # and predict the average perplexity score.\n",
    "    total_perplexity=0\n",
    "    for sent in test_data:\n",
    "        total_perplexity+=calc_perplexity(sent,mle_dict,n)\n",
    "    average_perplexity=-1*total_perplexity/len(test_data)\n",
    "    return average_perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 1\n",
      "\n",
      "\n",
      "Perplexity: 75.64742294533958\n",
      "\n",
      "\n",
      "Sentence  1 :  right them 17 that a more they a shooting speak think\n",
      "Sentence  2 :  truck not tortured inspect the it campaign\n",
      "Sentence  3 :  we has just are true do support japan they in out\n",
      "Sentence  4 :  held a\n",
      "Sentence  5 :  in your\n",
      "\n",
      "\n",
      "N: 2\n",
      "\n",
      "\n",
      "Perplexity: 31.96107387700408\n",
      "\n",
      "\n",
      "Sentence  1 :  they dont even this case but i was just youre talking about me .\n",
      "Sentence  2 :  i said are so much less .\n",
      "Sentence  3 :  were brining education to cost 900000 and he said skip iowa .\n",
      "Sentence  4 :  my opinion his donors to me the general petraeus was itbecause if you everybody thats right .\n",
      "Sentence  5 :  want a great guy and i mean she is going to be adversaries .\n",
      "\n",
      "\n",
      "N: 3\n",
      "\n",
      "\n",
      "Perplexity: 9.487992529018515\n",
      "\n",
      "\n",
      "Sentence  1 :  transpacific partnership .\n",
      "Sentence  2 :  actions along with everybody .\n",
      "Sentence  3 :  shes talking — i have no incentive to work .\n",
      "Sentence  4 :  guess i just want to see people i know its coming with these super pacs .\n",
      "Sentence  5 :  day one .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(1,4): # For unigram, bigram and trigram\n",
    "    print(\"N:\",n)\n",
    "    print(\"\\n\")\n",
    "    try:\n",
    "        mle_dict=pickle.load(open(f\"mle_dict_{n}.pkl\",\"rb\")) # If the mle_dict is already present load it\n",
    "    except:\n",
    "        mle_dict=ngram_mle(n,train_data) # Else, generate it using ngram_mle function and then save it\n",
    "        pickle.dump(mle_dict,open(f\"mle_dict_{n}.pkl\",\"wb\"))\n",
    "        \n",
    "    perplexity=calc_perplexity_test_data(test_data,mle_dict,n) # Calculate perplexity of this ngram over the test data \n",
    "    print(\"Perplexity:\",perplexity)\n",
    "    print(\"\\n\")\n",
    "    for i in range(5):  # Generate 5 sentences.\n",
    "        gen_sentence=generator(n,mle_dict)\n",
    "        print(\"Sentence \",i+1,\": \",\" \".join(gen_sentence[1:-1]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grammar of bigram and trigram is pretty good, but trigram lacks in predicitng a complete sentence while bigram does a much better job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a pre-trained word embedding layer for the network\n",
    "word_embeddings = gensim.models.Word2Vec(tokenized_sent_list,size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size,embedding_size=word_embeddings.wv.vectors.shape\n",
    "weights = word_embeddings.wv.vectors\n",
    "vocabulary=word_embeddings.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every sequence of word, store the next word as its output\n",
    "# If the sentence is w1 w2 w3 w4; \n",
    "# Training data would contain w1 -> w2; w1,w2 -> w3; w1,w2,w3 -> w4\n",
    "\n",
    "max_sent_length=100\n",
    "\n",
    "def get_traindata(tokenized_sent_list):\n",
    "    \n",
    "    x=np.array([])\n",
    "    y=np.array([])\n",
    "    k=[]\n",
    "    yall=[]\n",
    "    \n",
    "    for i in range(len(tokenized_sent_list)):\n",
    "        \n",
    "        sentence=tokenized_sent_list[i]\n",
    "        start=sentence[0]\n",
    "        l=[]  \n",
    "        for j in range(1,min(len(sentence),max_sent_length)):\n",
    "            x=vocabulary[sentence[j]].index\n",
    "            k.append(l+[0]*(max_sent_length-len(l)))\n",
    "            yall.append(vocabulary[sentence[j]].index)\n",
    "            l.append(x)\n",
    "        \n",
    "    return np.array(k),np.array(yall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(154418, 100) (154418,)\n"
     ]
    }
   ],
   "source": [
    "x,y=get_traindata(train_data)\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(np.reshape(np.array(list(vocabulary)),(-1,1)))\n",
    "y2=enc.transform(y.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/psychiquest/.local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocabulary_size, output_dim=embedding_size, weights=[weights]))\n",
    "model.add(LSTM(units=embedding_size))\n",
    "model.add(Dense(units=vocabulary_size))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model.load_weights(\"lstm_language_model_2.hdf5\")\n",
    "except:\n",
    "    filepath=\"weights-improvement-{epoch:02d}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    model.fit(x,y2,batch_size=128,epochs=3,callbacks=callbacks_list)\n",
    "    model.save(\"lstm_language_model_2.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(input_dim=vocabulary_size, output_dim=embedding_size, weights=[weights]))\n",
    "model2.add(SimpleRNN(units=embedding_size))\n",
    "model2.add(Dense(units=vocabulary_size))\n",
    "model2.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model2.load_weights(\"rnn.hdf5\")\n",
    "except:\n",
    "    filepath=\"weights-improvement-rnn-{epoch:02d}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    model2.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    model2.fit(x,y2,batch_size=128,epochs=1,callbacks=callbacks_list)\n",
    "    model2.save(\"rnn.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Generator\n",
    "\n",
    "def generator_neural(model):\n",
    "    prob = np.random.multinomial(1, [1/vocabulary_size]*vocabulary_size, 1)\n",
    "    startind=np.argmax(prob)\n",
    "    word_idxs=[startind]\n",
    "\n",
    "    while((word_idxs[-1]!=1) and (len(word_idxs)<25)):\n",
    "        preds = model.predict(x=np.array([word_idxs]))\n",
    "        preds[0]=np.divide(preds[0],1*sum(preds[0]))\n",
    "        prob = np.random.multinomial(1, preds[0],1)\n",
    "        idx = np.argmax(prob)\n",
    "        word_idxs.append(idx)\n",
    "\n",
    "    return (' '.join(word_embeddings.wv.index2word[idx] for idx in word_idxs[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Based Model:\n",
      "\n",
      "natos bombs 16500 moves poses raises taller lyin withering could—they\n",
      "ukraine readyi vaccine team evergrowing afterwards banking letting that\n",
      "police fully silicon releasing schuster jihad impressed\n",
      "guardian mathematics tractor prognosticator maine putt reelected greet trains reasonable citizen recover\n",
      "blown rallies break rolling door contractors chemistry inclined sentence phenomenon doill\n"
     ]
    }
   ],
   "source": [
    "print(\"LSTM Based Model:\\n\")\n",
    "for i in range(5):\n",
    "    print(generator_neural(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity score is calcualted as sum of log of conditional probabilities. When we give first x words as an input \n",
    "# to the model, we calculate the log probability of (x+1)th word and add it and then we find the mean perplexity over\n",
    "# the complete corpus\n",
    "\n",
    "def calc_perplexity(model,test_data):\n",
    "    perplexity=[]\n",
    "    for sent in test_data:\n",
    "        prob=0\n",
    "        start=sent[0]\n",
    "        x=[vocabulary[start].index]\n",
    "        for j in range(1,len(sent)):\n",
    "            true=sent[j]\n",
    "            trueindex=vocabulary[true].index\n",
    "            pred=model.predict(np.array(x))\n",
    "#             print(pred.shape)\n",
    "            prob1=pred[0][trueindex]\n",
    "            prob+=log(prob1)\n",
    "            x.append(trueindex)\n",
    "        perplexity.append(-1*prob/n)\n",
    "#         print(np.mean(perplexity))\n",
    "    return np.mean(perplexity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of LSTM Model 37.52834465890878\n",
      "Perplexity of RNN Model 56.27168870181293\n"
     ]
    }
   ],
   "source": [
    "print(\"Perplexity of LSTM Model\",calc_perplexity(model,test_data))\n",
    "print(\"Perplexity of RNN Model\",calc_perplexity(model2,test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perplexity score of LSTM and RNN model is greater than that of Bigram. But, these models were trained for very few epochs. If the model is trained for more epeochs, LSTM model will surely give better results than Bigram while RNN based model might not be that good. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
